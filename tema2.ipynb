{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/bin/bash\n",
        "!kaggle datasets download moltean/fruits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apZMftQ0DSZ-",
        "outputId": "00caa58d-db43-451c-ec0b-bec38d696cb5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Resize, Normalize, Compose\n",
        "from torchvision.transforms import v2\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import os\n",
        "import time\n",
        "\n",
        "VALIDATION = 0.2\n",
        "FASHION_BATCH_SIZE = 200\n",
        "matrix_transforms = v2.Compose([\n",
        "    ToTensor(),\n",
        "    Resize((64, 64)),\n",
        "    Normalize(0.5, 0.5),\n",
        "])\n",
        "\n",
        "\n",
        "class EarlyStopping():\n",
        "    def __init__(self, path : str, patience=5, threshold=1e-4):\n",
        "        self.patience = patience\n",
        "        self.threshold = threshold\n",
        "        self.min_loss = 10000\n",
        "        self.steps_till_stop = 0\n",
        "        self.path = path\n",
        "\n",
        "    def continue_training(self, model, loss):\n",
        "        if(loss < self.min_loss - self.threshold):\n",
        "            self.min_loss = loss\n",
        "            self.steps_till_stop = 0\n",
        "            torch.save(model.state_dict(), self.path)\n",
        "            return True\n",
        "        if (loss >= self.min_loss - self.threshold):\n",
        "            self.steps_till_stop += 1\n",
        "            if (self.steps_till_stop == self.patience): return False\n",
        "        return True\n",
        "    \n",
        "    def load_model(self, model):\n",
        "        model.load_state_dict(torch.load(self.path, weights_only=True))\n",
        "        return model\n",
        "\n",
        "\n",
        "\n",
        "def plot_losses(train_losses: list[float], validation_losses: list[float], label_x : str, label_y : str, title : str) -> None:\n",
        "    \"\"\"Plots training and validation losses over epochs.\"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(train_losses, label=label_x, marker=\"o\")\n",
        "    plt.plot(validation_losses, label=label_y, marker=\"o\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def get_fashion_dataloader(size : int):\n",
        "    train_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=matrix_transforms)\n",
        "    test_data = datasets.FashionMNIST(root='./data', train=False, download=True, transform=matrix_transforms)\n",
        "    train_dataloader = DataLoader(train_data, batch_size=size, shuffle=False)\n",
        "    test_dataloader = DataLoader(test_data, batch_size=size, shuffle=False)\n",
        "    return train_data, test_data, train_dataloader, test_dataloader\n",
        "\n",
        "train_fashion_dataset, test_fashion_dataset, train_fashion_dataloader, test_fashion_dataloader = get_fashion_dataloader(FASHION_BATCH_SIZE)\n",
        "FASHION_KEYS = np.unique(train_fashion_dataset.targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "AakH4z_IVDrI"
      },
      "outputs": [],
      "source": [
        "class InfoLoaderDataset(Dataset):\n",
        "    def __init__(self, data, labels, num_classes, transforms = None):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.num_classes = num_classes\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if (self.transforms == None):\n",
        "            return self.data[idx], self.labels[idx]\n",
        "        data = self.data[idx].detach().numpy()\n",
        "        return self.transforms(data), self.labels[idx]\n",
        "\n",
        "def initialiseTrainingModelHelpers(model, parameters):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=parameters['lr_optimizer'], weight_decay=parameters['weight_decay_factor'])\n",
        "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                    factor=parameters['gamma_lr'], patience=parameters['patience'], verbose=True)\n",
        "    return criterion, optimizer, lr_scheduler\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "    train_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels= []\n",
        "    for idx, (inputs, labels) in enumerate(dataloader):\n",
        "        inputs = inputs.type(torch.float32)\n",
        "        labels = labels.type(torch.LongTensor)\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        all_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "    return train_loss / len(dataloader), accuracy_score(all_preds, all_labels)\n",
        "\n",
        "def test(model, dataloader, criterion):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        validation_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels= []\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.type(torch.float32)\n",
        "            labels = labels.type(torch.LongTensor)\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            out = model(inputs)\n",
        "            validation_loss += criterion(out, labels).item()\n",
        "            all_preds.extend(torch.argmax(out, dim=1).cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "        return validation_loss / len(dataloader), accuracy_score(all_preds, all_labels), all_preds, all_labels\n",
        "\n",
        "def train_model(model, data, criterion, optimizer, lr_scheduler, epochs, early_stopper : EarlyStopping):\n",
        "    train_info = {\n",
        "        \"train_loss\": [],\n",
        "        \"validation_loss\" : [],\n",
        "        \"train_accuracy\": [],\n",
        "        \"validation_accuracy\" : []\n",
        "    }\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    max_acc = 0\n",
        "    for i in range(epochs):\n",
        "        train_loss, train_acc = train(model, data[\"train\"], optimizer, criterion)\n",
        "        val_loss, val_acc, _, _ = test(model, data[\"validation\"], criterion)\n",
        "        time.sleep(2)\n",
        "        if (val_acc > max_acc):\n",
        "            max_acc = val_acc\n",
        "            print(\"acuratete maxima la epoca\", i)\n",
        "        lr_scheduler.step(val_loss)\n",
        "        print(val_loss, val_acc, \"epoch: \", i)\n",
        "        train_info[\"train_loss\"].append(train_loss)\n",
        "        train_info[\"validation_loss\"].append(val_loss)\n",
        "        train_info[\"train_accuracy\"].append(train_acc)\n",
        "        train_info[\"validation_accuracy\"].append(val_acc)\n",
        "        if not early_stopper.continue_training(model, val_loss): break\n",
        "    return train_info\n",
        "\n",
        "def test_model(model, dataloader, criterion):\n",
        "    loss, accuracy, pred, labels = test(model, dataloader, criterion)\n",
        "    print(\"loss was:\", loss, \" accuracy was:\", accuracy)\n",
        "    print()\n",
        "    out = {\n",
        "        \"f1\": f1_score(labels, pred, labels=range(0, 10), average='macro'),\n",
        "        \"recall\": recall_score(labels, pred, labels=range(0, 10), average='macro'),\n",
        "        \"precision\": precision_score(labels, pred, labels=range(0, 10), average='macro'),\n",
        "        \"accuracy\": accuracy,\n",
        "    }\n",
        "    print(out)\n",
        "    return out, confusion_matrix(labels, pred),\n",
        "\n",
        "\n",
        "def execute_pipeline(model, data, parameters, early_stopper):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    criterion, optimizer, lr_scheduler = initialiseTrainingModelHelpers(model, parameters)\n",
        "    train_info = train_model(model, data, criterion, optimizer,\n",
        "                             lr_scheduler, parameters['epochs'], early_stopper)\n",
        "    plot_losses(train_info[\"train_loss\"], train_info[\"validation_loss\"], \n",
        "                \"Train Loss\", \"Validation Loss\", \"Training and Validation Loss\")\n",
        "    plot_losses(train_info[\"train_accuracy\"], train_info[\"validation_accuracy\"], \n",
        "                \"Train Accuracy\", \"Validation Accuracy\", \"Training and Validation Accuracy\")\n",
        "    model = early_stopper.load_model(model)\n",
        "    return test_model(model, data[\"test\"], criterion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwUMLPrbkBW-",
        "outputId": "768bdd23-3477-4ee4-ebf8-ba2d9845aea9"
      },
      "outputs": [],
      "source": [
        "# Task 1\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "from task1_aux import compute_vocabulary, create_initial_features, compute_attributes\n",
        "pd.set_option(\"display.precision\", 15)\n",
        "SIZE = (28, 28)\n",
        "BATCH_SIZE = 100\n",
        "# DE ADAUGAT TRANSFORMARI\n",
        "vocabulary = compute_vocabulary(train_fashion_dataloader, 5)\n",
        "print(vocabulary.shape)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "X_train, y_train = create_initial_features(train_fashion_dataloader, vocabulary, FASHION_BATCH_SIZE)\n",
        "X_test, y_test = create_initial_features(test_fashion_dataloader, vocabulary, FASHION_BATCH_SIZE)\n",
        "rf.fit(X_train, y_train)\n",
        "accuracy_before = rf.score(X_test, y_test)\n",
        "print(f'accuracy after one more step', accuracy_before)\n",
        "importances = rf.feature_importances_\n",
        "feature_names = range(vocabulary.shape[0])\n",
        "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "top_features = feature_importance_df['Feature'][:64].values\n",
        "vocabulary = vocabulary[top_features]\n",
        "X_train, y_train, X_validation, y_validation, X_test, y_test = compute_attributes(vocabulary,\n",
        "                            train_fashion_dataloader,  test_fashion_dataloader, FASHION_BATCH_SIZE, FASHION_KEYS, VALIDATION)\n",
        "data = {\n",
        "    \"train\":  DataLoader(InfoLoaderDataset(X_train, y_train, 10), BATCH_SIZE, shuffle=True),\n",
        "    \"validation\": DataLoader(InfoLoaderDataset(X_validation, y_validation, 10), BATCH_SIZE, shuffle=True),\n",
        "    \"test\": DataLoader(InfoLoaderDataset(X_test, y_test, 10), BATCH_SIZE, shuffle=True),\n",
        "}\n",
        "print(vocabulary.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "zC_D8qz_ee7y",
        "outputId": "567f9406-97c9-4f1c-cbf3-22f0bea1b7d8"
      },
      "outputs": [],
      "source": [
        "class FeaturesMLP(nn.Module):\n",
        "    def __init__(self, expansion, initial_features, num_classes=10):\n",
        "        super(FeaturesMLP, self).__init__()\n",
        "        width1 = initial_features\n",
        "        width2  = width1 * expansion\n",
        "        self.fc1 = nn.Linear(width1, width2)\n",
        "        self.bn1 = nn.BatchNorm1d(width2)\n",
        "\n",
        "        width3 = width2 * expansion\n",
        "        self.fc2 = nn.Linear(width2, width3)\n",
        "        self.bn2 = nn.BatchNorm1d(width3)\n",
        "\n",
        "        self.fc3 = nn.Linear(width3, width2)\n",
        "        self.bn3 = nn.BatchNorm1d(width2)\n",
        "\n",
        "        self.fc4 = nn.Linear(width2, width1)\n",
        "        self.bn4 = nn.BatchNorm1d(width1)\n",
        "\n",
        "        self.fc5 = nn.Linear(width1, num_classes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.bn1(self.relu(self.fc1(x)))\n",
        "        x = self.bn2(self.relu(self.fc2(x)))\n",
        "        x = self.bn3(self.relu(self.fc3(x)))\n",
        "        x = self.bn4(self.relu(self.fc4(x)))\n",
        "        return self.softmax(self.fc5(x))\n",
        "\n",
        "parameters = {\n",
        "    'expansion': 4,\n",
        "    'epochs' : 10,\n",
        "    'lr_optimizer': 1e-4,\n",
        "    'gamma_lr': 0.7,\n",
        "    'weight_decay_factor': 1e-4,\n",
        "    'patience': 3,\n",
        "    'early_stopper': 6,\n",
        "}\n",
        "early_stopper = EarlyStopping(\"./task1.pth\", parameters['early_stopper'])\n",
        "model = FeaturesMLP(expansion=parameters['expansion'], initial_features=64)\n",
        "results_task1_fashion, task1_mat_fashion = execute_pipeline(model, data, parameters, early_stopper)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=task1_mat_fashion, display_labels=range(0, 10))\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdLQOMa6VDrI"
      },
      "outputs": [],
      "source": [
        "# Task 2\n",
        "from task1_aux import split_data_in_training_validation\n",
        "SIZE = (28, 28)\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "class FullImageMLP(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(FullImageMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 250)\n",
        "        self.bn1 = nn.BatchNorm1d(250)\n",
        "        self.fc2 = nn.Linear(250, 250)\n",
        "        self.bn2 = nn.BatchNorm1d(250)\n",
        "        self.fc3 = nn.Linear(250, num_classes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.bn2(x)\n",
        "        return self.softmax(self.fc3(x))\n",
        "\n",
        "transforms = v2.Compose([\n",
        "    ToTensor(),\n",
        "    Resize(SIZE),\n",
        "    Normalize(0.5, 0.5),\n",
        "])\n",
        "\n",
        "X_train, y_train, X_validation, y_validation = split_data_in_training_validation(train_fashion_dataset.data,\n",
        "                                                    train_fashion_dataset.targets, FASHION_KEYS, VALIDATION)\n",
        "X_test, y_test = test_fashion_dataset.data, test_fashion_dataset.targets\n",
        "data = {\n",
        "    \"train\" : DataLoader(InfoLoaderDataset(X_train, y_train, 10, transforms), BATCH_SIZE, shuffle=True),\n",
        "    \"validation\": DataLoader(InfoLoaderDataset(X_validation, y_validation, 10, transforms), BATCH_SIZE, shuffle=True),\n",
        "    \"test\": DataLoader(InfoLoaderDataset(X_test, y_test, 10, transforms), BATCH_SIZE, shuffle=True)\n",
        "}\n",
        "\n",
        "model = FullImageMLP()\n",
        "parameters = {\n",
        "    'epochs' : 3,\n",
        "    'lr_optimizer': 1e-4,\n",
        "    'gamma_lr': 0.5,\n",
        "    'weight_decay_factor': 1e-4,\n",
        "    'patience': 3,\n",
        "    'early_stopper': 6,\n",
        "}\n",
        "early_stopper =  EarlyStopping(\"./task2.pth\", parameters['early_stopper'])\n",
        "results_task2_fashion, task2_mat_fashion = execute_pipeline(model, data, parameters, early_stopper)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=task2_mat_fashion, display_labels=range(0, 10))\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rYu_LqmSkBXB"
      },
      "outputs": [],
      "source": [
        "# Task 3\n",
        "from task1_aux import split_data_in_training_validation\n",
        "SIZE = (28, 28)\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "training_transforms = v2.Compose([\n",
        "    ToTensor(),\n",
        "    v2.RandomHorizontalFlip(),\n",
        "    v2.RandomRotation(degrees=90),\n",
        "    v2.RandomCrop(\n",
        "      size=(SIZE),\n",
        "      padding=(4, 4),\n",
        "      padding_mode=\"reflect\"),\n",
        "    Resize(SIZE),\n",
        "    Normalize(0.5, 0.5)\n",
        "])\n",
        "\n",
        "testing_transforms = v2.Compose([\n",
        "    ToTensor(),\n",
        "    Resize(SIZE),\n",
        "    Normalize(0.5, 0.5)\n",
        "])\n",
        "\n",
        "X_train, y_train, X_validation, y_validation = split_data_in_training_validation(train_fashion_dataset.data,\n",
        "                                                    train_fashion_dataset.targets, FASHION_KEYS, VALIDATION)\n",
        "X_test, y_test = test_fashion_dataset.data, test_fashion_dataset.targets\n",
        "data = {\n",
        "    \"train\": DataLoader(InfoLoaderDataset(X_train, y_train, 10, training_transforms), BATCH_SIZE, shuffle=True),\n",
        "    \"validation\": DataLoader(InfoLoaderDataset(X_validation, y_validation, 10, testing_transforms), BATCH_SIZE, shuffle=True),\n",
        "    \"test\":  DataLoader(InfoLoaderDataset(X_test, y_test, 10, testing_transforms), BATCH_SIZE, shuffle=True),\n",
        "}\n",
        "\n",
        "data_no_transform = {\n",
        "    \"train\": DataLoader(InfoLoaderDataset(X_train, y_train, 10), BATCH_SIZE, shuffle=True),\n",
        "    \"validation\": DataLoader(InfoLoaderDataset(X_validation, y_validation, 10), BATCH_SIZE, shuffle=True),\n",
        "    \"test\":  DataLoader(InfoLoaderDataset(X_test, y_test, 10), BATCH_SIZE, shuffle=True),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "n7huC0bmkBXB"
      },
      "outputs": [],
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1, padding=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=padding, bias=False)\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, bottleneck_channels, block_channels, norm_layer):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv1 = conv1x1(block_channels, bottleneck_channels)\n",
        "        self.norm1 = norm_layer(bottleneck_channels)\n",
        "        self.conv2 = conv3x3(bottleneck_channels, bottleneck_channels)\n",
        "        self.norm2 = norm_layer(bottleneck_channels)\n",
        "        self.conv3 = conv1x1(bottleneck_channels, block_channels)\n",
        "        self.norm3 = norm_layer(block_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.norm1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.norm2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.norm3(out)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class DeepConvNet(nn.Module):\n",
        "    def __init__(self, base_depth, expansion, nr_layer_blocks, num_classes=10):\n",
        "        super(DeepConvNet, self).__init__()\n",
        "        self.base_depth = base_depth\n",
        "        # convert image to grayscale before applying transformation\n",
        "        self.conv1 = nn.Conv2d(1, self.base_depth, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        depth1 = self.base_depth * expansion\n",
        "        self.conv_depth1 = conv1x1(self.base_depth, depth1, stride=1)\n",
        "        self.norm_depth1 = nn.BatchNorm2d(depth1)\n",
        "        # # #### Layer 1, 3 blocks\n",
        "        self.layer1 = nn.Sequential(\n",
        "            *[ConvBlock(self.base_depth, depth1, nn.BatchNorm2d)] * nr_layer_blocks\n",
        "        )\n",
        "\n",
        "        # # change to 128 channels base and image size 8x8\n",
        "        depth2 = depth1 * expansion\n",
        "        self.conv_depth2 = conv1x1(depth1, depth2, 2)\n",
        "        self.norm_depth2 = nn.BatchNorm2d(depth2)\n",
        "\n",
        "        # #### Layer 2, 3 blocks\n",
        "        self.layer2 = nn.Sequential(\n",
        "            *[ConvBlock(depth1, depth2, nn.BatchNorm2d)] * nr_layer_blocks\n",
        "        )\n",
        "\n",
        "        # change to 256 chanels and image size 4x4\n",
        "        depth3 = depth2 * expansion\n",
        "        self.conv_depth3 = conv1x1(depth2, depth3, 2)\n",
        "        self.norm_depth3 = nn.BatchNorm2d(depth3)\n",
        "\n",
        "        # Layer 3, 3 blocks\n",
        "        self.layer3 = nn.Sequential(\n",
        "            *[ConvBlock(depth2, depth3, nn.BatchNorm2d)] * nr_layer_blocks\n",
        "        )\n",
        "\n",
        "        depth4 = depth3 * expansion\n",
        "        self.conv_depth4 = conv1x1(depth3, depth4, 2)\n",
        "        self.norm_depth4 = nn.BatchNorm2d(depth4)\n",
        "        # self.layer4 = nn.Sequential(\n",
        "        #     *[ConvBlock(depth3, depth4, nn.BatchNorm2d)] * nr_layer_blocks\n",
        "        # )\n",
        "\n",
        "        # # AveragePooling\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classification_layer = nn.Sequential(*[\n",
        "            # nn.Dropout(0.3),\n",
        "            nn.Linear(depth4, depth2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(depth2, num_classes)\n",
        "        ])\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.conv_depth1(x)\n",
        "        x = self.norm_depth1(x)\n",
        "        x = self.layer1(x)\n",
        "\n",
        "        x = self.relu(self.conv_depth2(x))\n",
        "        x = self.norm_depth2(x)\n",
        "        x = self.layer2(x)\n",
        "\n",
        "        x = self.relu(self.conv_depth3(x))\n",
        "        x = self.norm_depth3(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.relu(self.conv_depth4(x))\n",
        "        x = self.norm_depth4(x)\n",
        "        # x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        # de la clasification layer\n",
        "        return self.classification_layer(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrvhaIPSkBXC",
        "outputId": "2dd5cee1-576d-4e29-a3b8-8cfa6e98f8e7"
      },
      "outputs": [],
      "source": [
        "parameters = {\n",
        "    'epochs' : 2,\n",
        "    'lr_optimizer': 1e-4,\n",
        "    'gamma_lr': 0.1,\n",
        "    'weight_decay_factor': 1e-4,\n",
        "    'patience': 4,\n",
        "    'base_depth': 16,\n",
        "    'expansion': 4,\n",
        "    'nr_layers_block': 2,\n",
        "    'early_stopping' : 9\n",
        "}\n",
        "\n",
        "model = DeepConvNet(base_depth=parameters['base_depth'], expansion=parameters['expansion'], \n",
        "                    nr_layer_blocks=parameters['nr_layers_block'])\n",
        "early_stopper =  EarlyStopping(\"./task3_transforms.pth\", parameters['early_stopping'])\n",
        "result, cmat = execute_pipeline(model, data, parameters, early_stopper)\n",
        "results_task3_transformations_fashion, task3_transformation_mat_fashion = execute_pipeline(model, data, parameters, early_stopper)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=task3_transformation_mat_fashion, display_labels=range(0, 10))\n",
        "disp.plot()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_QH_uuNee72"
      },
      "outputs": [],
      "source": [
        "parameters = {\n",
        "    'epochs' : 2,\n",
        "    'lr_optimizer': 1e-4,\n",
        "    'gamma_lr': 0.1,\n",
        "    'weight_decay_factor': 1e-4,\n",
        "    'patience': 4,\n",
        "    'base_depth': 16,\n",
        "    'expansion': 4,\n",
        "    'nr_layers_block': 2,\n",
        "    'early_stopping' : 9\n",
        "}\n",
        "model = DeepConvNet(base_depth=parameters['base_depth'], expansion=parameters['expansion'], \n",
        "                    nr_layer_blocks=parameters['nr_layers_block'])\n",
        "early_stopper =  EarlyStopping(\"./task3_transforms.pth\", parameters['early_stopping'])\n",
        "execute_pipeline(model, data, parameters, early_stopper)\n",
        "results_task3_no_transformations_fashion, task3_no_transformation_mat_fashion = execute_pipeline(model, data, parameters, early_stopper)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=task3_no_transformation_mat_fashion, display_labels=range(0, 10))\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 4\n",
        "from resnet import resnet18\n",
        "model = resnet18()\n",
        "model.load_state_dict(torch.load(\"./resnet18.pt\", weights_only=True))\n",
        "SIZE = (32, 32)\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "training_task_4 = v2.Compose([\n",
        "    ToTensor(),\n",
        "    Resize(SIZE),\n",
        "    v2.RandomHorizontalFlip(),\n",
        "    v2.RandomRotation(degrees=90),\n",
        "    v2.RandomCrop(\n",
        "      size=(SIZE),\n",
        "      padding=(4, 4),\n",
        "      padding_mode=\"reflect\"),\n",
        "    Normalize(mean = [0.4914, 0.4822, 0.4465], std = [0.2471, 0.2435, 0.2616]),\n",
        "])\n",
        "\n",
        "testing_task_4 = v2.Compose([\n",
        "    ToTensor(),\n",
        "    Resize(SIZE),\n",
        "    Normalize(mean = [0.4914, 0.4822, 0.4465], std = [0.2471, 0.2435, 0.2616]),\n",
        "])\n",
        "\n",
        "def training_transforms(image : np.array):\n",
        "    image = np.expand_dims(image, axis=2)\n",
        "    image = np.concatenate((image, image, image), axis=2)\n",
        "    return training_task_4(image)\n",
        "\n",
        "def testing_transforms(image : np.array):\n",
        "    image = np.expand_dims(image, axis=2)\n",
        "    image = np.concatenate((image, image, image), axis=2)\n",
        "    return testing_task_4(image)\n",
        "\n",
        "\n",
        "X_train, y_train, X_validation, y_validation = split_data_in_training_validation(train_fashion_dataset.data,\n",
        "                                                    train_fashion_dataset.targets, FASHION_KEYS, VALIDATION)\n",
        "X_test, y_test = test_fashion_dataset.data, test_fashion_dataset.targets\n",
        "data = {\n",
        "    \"train\": DataLoader(InfoLoaderDataset(X_train, y_train, 10, training_transforms), BATCH_SIZE, shuffle=True),\n",
        "    \"validation\": DataLoader(InfoLoaderDataset(X_validation, y_validation, 10, testing_transforms), BATCH_SIZE, shuffle=True),\n",
        "    \"test\":  DataLoader(InfoLoaderDataset(X_test, y_test, 10, testing_transforms), BATCH_SIZE, shuffle=True),\n",
        "}\n",
        "\n",
        "parameters = {\n",
        "    'epochs' : 2,\n",
        "    'lr_optimizer': 1e-4,\n",
        "    'gamma_lr': 0.4,\n",
        "    'weight_decay_factor': 1e-4,\n",
        "    'patience': 3,\n",
        "    'base_depth': 16,\n",
        "    'early_stopping': 5, # inainte 9\n",
        "    'momentum': 0.5,\n",
        "}\n",
        "\n",
        "early_stopper =  EarlyStopping(\"./task4.pth\", parameters['early_stopping'])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=parameters['lr_optimizer'], momentum=parameters['momentum'],\n",
        "                             weight_decay=parameters['weight_decay_factor'])\n",
        "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                    factor=parameters['gamma_lr'], patience=parameters['patience'], verbose=True)\n",
        "train_info = train_model(model, data, criterion, optimizer,\n",
        "                             lr_scheduler, parameters['epochs'], early_stopper)\n",
        "plot_losses(train_info[\"train_loss\"], train_info[\"validation_loss\"], \"Train Loss\", \"Validation Loss\", \"Training and Validation Loss\")\n",
        "plot_losses(train_info[\"train_accuracy\"], train_info[\"validation_accuracy\"], \"Train Accuracy\", \"Validation Accuracy\", \"Training and Validation Accuraacy\")\n",
        "model = early_stopper.load_model(model)\n",
        "results_task4_fashion, task4_mat_fashion = test_model(model, data[\"test\"], criterion)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=task4_mat_fashion, display_labels=range(0, 10))\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "def compute_results():\n",
        "    all_info = [results_task2_fashion, results_task3_transformations_fashion,\n",
        "                results_task3_no_transformations_fashion, results_task4_fashion]\n",
        "    data = {\n",
        "        # \"Features MLP\",\n",
        "        \"names\": [\"MLP Full Image\", \"DeepConv Trasformation\", \"DeepConv No Transforms\", \"Pretrained Resnet18\"],\n",
        "        \"f1\": np.zeros(len(all_info)),\n",
        "        \"recall\": np.zeros(len(all_info)),\n",
        "        \"precision\": np.zeros(len(all_info)),\n",
        "        \"accuracy\": np.zeros(len(all_info)),\n",
        "    }\n",
        "\n",
        "    for idx, info in enumerate(all_info):\n",
        "        data[\"f1\"][idx] = info[\"f1\"]\n",
        "        data[\"recall\"][idx] = info[\"recall\"]\n",
        "        data[\"precision\"][idx] = info[\"precision\"]\n",
        "        data[\"accuracy\"][idx] = info[\"accuracy\"]\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "df = compute_results()\n",
        "df"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
